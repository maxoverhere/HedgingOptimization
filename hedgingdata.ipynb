{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hedgingdata.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPREBgks5NEQ3/wf255Iz1d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxoverhere/HedgingOptimization/blob/main/hedgingdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qCOYYwDo0Fk"
      },
      "source": [
        "#Constants\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXA023hLo9Va"
      },
      "source": [
        "RUN = False # create graphs and run tests\n",
        "COLAB = True # running on google colab (afects save and load file locations)\n",
        "\n",
        "# data generation config\n",
        "MAX_VOLUME = 5\n",
        "DAY_LEN = 1440\n",
        "NUM_CCYS = 3\n",
        "\n",
        "# environment config \n",
        "NUM_BUCKETS = 12\n",
        "ACTION_BUCKETS = 2\n",
        "NUM_TIME_PERIODS = 24 * 3\n",
        "LEN_TIME_PERIOD = DAY_LEN // NUM_TIME_PERIODS\n",
        "SMALLEST_BUCKET = 1\n",
        "NUM_WAIT_BUCKET = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_AdgBTQmjkB"
      },
      "source": [
        "#Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA3JPGSzjS6F"
      },
      "source": [
        "The data will be generated at runtime so no fat training files needed. Also all the training data is totally reproucable as genrated from a seed. What is needed though is the method that is to be used to generate the data. \n",
        "\n",
        "100 days will be a run, so this data will be generated to match that. The data will be based arround the behavior of a number of clients so the data that is need to train is the features of these clients. The clients that are generated will be generated so as to reflect a number of currencies and each currencies charactaristics.\n",
        "\n",
        "###Client Charactaristics\n",
        "\n",
        "variable | value | meaning\n",
        "--- | --- | ---\n",
        "buyCcy | 0 - 9 | the currency they buy from the brokerage\n",
        "sellCcy | 0 - 9 | the currency they sell from the brokerage\n",
        "trading_time | 0 - 1440 | minute of the day the client is most likely to trade at\n",
        "volume | 0 - 5 | the logatithmic total volume a clients trades\n",
        "frequency | 0 - 2 | the frequency of client trades from 0 being once a week to 3 being many trades a day\n",
        "time_regular | true - false | the regularity of client trade times\n",
        "volume_regular | true - false | the regularity of client trade volumes\n",
        "\n",
        "###Currency Charactaristics\n",
        "\n",
        "variable | value | meaning\n",
        "--- | --- | ---\n",
        "id | 0 - 9 | the identity, will be in decending order with respect to volume traded\n",
        "volume | 0 - 5 | the amount a currency is traded (logorithmic)\n",
        "trading_hour | 0 - 23 | the hour that is most likely for a client based in that country to be trading at\n",
        "\n",
        "###Currency Bias\n",
        "\n",
        "An upper traingular matrix storing the chance of a generated client exhanging the two currenices will take a particular side\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAPZy8zYym-d"
      },
      "source": [
        "## Currency Data\n",
        "The data will have two hubs to emulate the way that there are hubs that work at differnt times, like Asia having peek trading at diffrent hours to the Americas.\n",
        "\n",
        "To emulate this I will have one group arround 8:00 and the other at 15:00 with the biggest two currencies in terms of volume being in diffrent groups.\n",
        "\n",
        "There will be two major currencies and these currencies will have a volume size 5 and 4 with all other currencies having smaller volumes.\n",
        "\n",
        "If the volume of a currency is 5 it means the chance of that currency being used in a random transaction as compared to a currency of volume 0 should be a ration of 32 (2 raised by 5) : 1. Note that volume has no effect on the average size of each transaction in that currency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXzGWJivmz60"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(3)\n",
        "\n",
        "@dataclass\n",
        "class Currency:\n",
        "  volume : int\n",
        "  trading_time : int\n",
        "  id : int = 0\n",
        "\n",
        "def get_currencies():\n",
        "  # the two biggest currencies\n",
        "  currencies = [Currency(volume=5, trading_time=8*60), \n",
        "                Currency(volume=4, trading_time=14*60)] \n",
        "  \n",
        "  timezones = [8*60, 15*60]\n",
        "  for i in range(NUM_CCYS - 2):\n",
        "    currencies.append(Currency(volume=np.random.randint(0, MAX_VOLUME - 1),\n",
        "                               trading_time=timezones[i % 2] + np.random.randint(-2*60, 2*60)))\n",
        "    \n",
        "\n",
        "  currencies.sort(key=lambda x: x.volume, reverse=True)\n",
        "  currencies = np.array(currencies)\n",
        "  for i in range(NUM_CCYS):\n",
        "    currencies[i].id = i\n",
        "  return currencies\n",
        "\n",
        "currencies = get_currencies()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKMLRLWea_6T"
      },
      "source": [
        "if RUN:\n",
        "  plt.scatter(x=list(map(lambda x: x.trading_time/60, currencies)), y=list(map(lambda x: x.volume,currencies)))\n",
        "  plt.xlabel('hour of day')\n",
        "  plt.ylabel('currency volume')\n",
        "  plt.title(\"Currency peak trading time vs volume traded\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-9gbsaDVTob"
      },
      "source": [
        "##Currency Relations\n",
        "In reality not all relationships are equal, the amount bought and sold in a currency are not always the same due to the nature of countries speciailising in diffrent industries like tourism, heavy industry or natural resource production.\n",
        "\n",
        "To represent this I will have a 2d upper triangular matrix representing the relationship between each currency and a function that will return the relation between two currencies, returning the inverse if the currencies are swapped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeZjFvCCWJQt"
      },
      "source": [
        "currency_matrix = np.random.choice(a=[.25, .4, .5, .6, .75], size=(NUM_CCYS, NUM_CCYS))\n",
        "\n",
        "for point in [(x, y) for x in range(NUM_CCYS) for y in range(NUM_CCYS) if x <= y]:\n",
        "  currency_matrix[point[0], point[1]] = 0\n",
        "\n",
        "def get_ccy_bias(ccy1: int, ccy2: int):\n",
        "  return currency_matrix[ccy1, ccy2] if ccy1 > ccy2 else 1 - currency_matrix[ccy2, ccy1]\n",
        "\n",
        "if RUN:\n",
        "  print(currency_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adlx11aWIgvd"
      },
      "source": [
        "##Client Data\n",
        "Each client will be associated with a currency and be bias towards acting in that currencies trading hours. They will have a target target currency but will not be affected by that currencies trading hours. This is to reflect that in the real world a company is generally based in one country and will do their trading in that countries trading hours, due to people being awake at that time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnExaUqfLHDI"
      },
      "source": [
        "@dataclass\n",
        "class Client:\n",
        "  buy_ccy: int\n",
        "  sell_ccy: int\n",
        "  trading_time: int \n",
        "  volume: int \n",
        "  frequency: int\n",
        "  time_regular: bool\n",
        "  volume_regular: bool\n",
        "\n",
        "def get_clients():\n",
        "  def get_buy_sell_ccy(ccy: int, num_clients: int):\n",
        "    ccys = list(range(NUM_CCYS))\n",
        "    ccys.remove(ccy)\n",
        "    total_vol = sum(map(lambda x: 2 ** currencies[x].volume, ccys))\n",
        "    probablities = list(map(lambda x: 2 ** currencies[x].volume/total_vol, ccys))\n",
        "    matching_ccys = np.random.choice(a=ccys, size=num_clients, p=probablities)\n",
        "    directions = map(lambda x: np.random.random() < get_ccy_bias(ccy, x), matching_ccys)\n",
        "    matching_ccy_and_direction = list(zip(iter(matching_ccys), directions))\n",
        "    sell_ccys = np.fromiter(map(lambda x: ccy if x[1] else x[0], matching_ccy_and_direction), int)\n",
        "    buy_ccys = np.fromiter(map(lambda x: x[0] if x[1] else ccy, matching_ccy_and_direction), int)\n",
        "    return buy_ccys, sell_ccys\n",
        "\n",
        "  def get_direction(num_clients: int):\n",
        "    return np.random.choice(a=[True, False], size=num_clients, p=[.5,.5])\n",
        "\n",
        "  def get_random_with_default(default_num: int, num_clients: int):\n",
        "    random = np.random.randint(0, 1000, num_clients)\n",
        "    return [x if x < END_DAY else default_num for x in random]\n",
        "\n",
        "  def get_trading_time(ccy: int, num_clients: int):\n",
        "    num_clients_not_normal = int(num_clients/50)\n",
        "    times = np.random.normal(loc=currencies[ccy].trading_time, scale=100, size=num_clients-num_clients_not_normal)\n",
        "    return np.append(times.astype(int), np.random.randint(low=0, high=DAY_LEN, size=num_clients_not_normal))\n",
        "\n",
        "  def get_trading_volumes(num_clients: int):\n",
        "    return np.random.choice(a=[0,1,2,3], size=num_clients, p=[.5,.3,.15,.05])\n",
        "\n",
        "  def get_frequencies(num_clients: int):\n",
        "    return np.random.choice(a=[0,1,2], size=num_clients, p=[.4,.3,.3])\n",
        "\n",
        "  def get_regularities(num_clients: int):\n",
        "    return np.random.choice(a=[True,False], size=num_clients, p=[.5,.5])\n",
        "\n",
        "  clients = np.array([Client(buy_ccy=0, sell_ccy=1, trading_time=14*60,volume=5, frequency=1, volume_regular=True, time_regular=True)])\n",
        "\n",
        "  for ccy in range(NUM_CCYS):\n",
        "    num_clients = 10 * 2 ** currencies[ccy].volume\n",
        "    buy_ccys, sell_ccys = get_buy_sell_ccy(ccy=ccy, num_clients=num_clients)\n",
        "    directions = get_direction(num_clients=num_clients)\n",
        "    trading_times = get_trading_time(ccy=ccy, num_clients=num_clients)\n",
        "    volumes = get_trading_volumes(num_clients=num_clients)\n",
        "    frequencies = get_frequencies(num_clients=num_clients)\n",
        "    time_regularities = get_regularities(num_clients=num_clients)\n",
        "    volume_regularities = get_regularities(num_clients=num_clients)\n",
        "    new_clients = np.array([Client(buy_ccy=buy_ccys[i], sell_ccy=sell_ccys[i],\n",
        "                                  trading_time=trading_times[i], volume=volumes[i], frequency=frequencies[i],\n",
        "                                  time_regular=time_regularities[i], volume_regular=volume_regularities[i])\n",
        "                            for i in range(num_clients)])\n",
        "\n",
        "    clients = np.append(clients, new_clients)\n",
        "  return clients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5OxpKFdbHmx"
      },
      "source": [
        "if RUN:\n",
        "  clients = get_clients()\n",
        "  print(\"generated \" + str(len(clients)) + \" clients in acordance with desired patterns\\n\")\n",
        "\n",
        "  plt.hist(list(map(lambda x: x.trading_time/60, clients)), bins=24*2)\n",
        "  plt.xlabel('hour of day')\n",
        "  plt.ylabel('frequency per 30 mins')\n",
        "  plt.title(\"trading time of clients frequency\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  plt.hist(list(map(lambda x: x.trading_time/60, \n",
        "                    clients[np.fromiter(map(lambda x: x.buy_ccy == 1 or x.sell_ccy == 1, clients), bool)])), bins=24*2)\n",
        "  plt.xlabel('hour of day')\n",
        "  plt.ylabel('frequency per 30 mins')\n",
        "  plt.title(\"trading time of clients trading currency 1\")\n",
        "  plt.show()\n",
        "\n",
        "  from collections import Counter\n",
        "  ccy_volumes_dict = list(Counter(map(lambda x: (x.buy_ccy, x.sell_ccy, x.volume), clients)).items())\n",
        "\n",
        "  ccy_buy_totals = np.zeros(NUM_CCYS)\n",
        "  ccy_sell_totals = np.zeros(NUM_CCYS)\n",
        "  ccy0_buy_totals = np.zeros(NUM_CCYS - 1)\n",
        "  ccy0_sell_totals = np.zeros(NUM_CCYS - 1)\n",
        "\n",
        "  for i in ccy_volumes_dict:\n",
        "    ccy_buy_totals[i[0][0]] += i[1] * (2**i[0][2])\n",
        "    ccy_sell_totals[i[0][1]] += i[1] * (2**i[0][2])\n",
        "    if i[0][0] == 0:\n",
        "      ccy0_buy_totals[i[0][1]-1] += i[1] * (2**i[0][2])\n",
        "    if i[0][1] == 0:\n",
        "      ccy0_sell_totals[i[0][0]-1] += i[1] * (2**i[0][2])\n",
        "\n",
        "  plt.title(\"Volume of each currency purchased\")\n",
        "  plt.pie(x=ccy_buy_totals, labels=range(NUM_CCYS));\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Volume of each currency sold\")\n",
        "  plt.pie(x=ccy_sell_totals, labels=range(NUM_CCYS));\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Volume of currency 0 purchased in each currency\")\n",
        "  plt.pie(x=ccy0_buy_totals, labels=range(1, NUM_CCYS));\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Volume of currency 0 sold in each currency\")\n",
        "  plt.pie(x=ccy0_sell_totals, labels=range(1, NUM_CCYS));\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmqkM_f5JFew"
      },
      "source": [
        "##Generating Day\n",
        "Each day will be generated by using the client info to build a number of trades in a day.\n",
        "\n",
        "There are two functions to get a day, one which just returns a list of postion changes throughout the day intended for the AI and another which returns a list of client trades that will be needed for the algorithmic benchmarking algorithem. The benchmarking algorithem is to nieve and needs this information as it cannot otherwise know what currency to hedge against."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIZLXDlPeH9A"
      },
      "source": [
        "class Day:\n",
        "  def __init__(self, clients):\n",
        "    self.clients = clients\n",
        "\n",
        "  def get_clients_active(self, day: int):\n",
        "    return self.clients[list(map(\n",
        "        lambda x: (x.frequency != 0 or np.random.random() < 0.25) and np.random.random() < 0.98\n",
        "        ,self.clients))]\n",
        "\n",
        "  def get_times(self, trading_time, time_regular: bool, frequency: int):\n",
        "    num_times = 1 if frequency < 2 else np.random.randint(0, 20)\n",
        "    scale = 10 if time_regular else 120\n",
        "    return np.array([int(np.random.normal(loc=trading_time, scale=scale)) % 1440 for _ in range(num_times)])\n",
        "\n",
        "  def get_volumes(self, num_trades: int, frequency: int, volume_regular: bool, volume: int):\n",
        "    distribution = .6, 1.4 if volume_regular else 0, 2\n",
        "    frequency_scaler = 10 if frequency < 2 else 1\n",
        "    return np.array([int(np.random.uniform(distribution[0], distribution[1])* (2 ** volume) * frequency_scaler)\n",
        "     for _ in range(num_trades)], dtype=np.int32)\n",
        "\n",
        "\n",
        "  def get_client_trades(self, day_num: int, client: Client):\n",
        "    times = self.get_times(client.trading_time, client.time_regular, client.frequency)\n",
        "    volumes = self.get_volumes(len(times), client.frequency, client.volume_regular, client.volume)\n",
        "    return times, volumes\n",
        "\n",
        "  def next_clients(self, day_num: int, use_seed: bool = False):\n",
        "    if use_seed:\n",
        "      np.random.seed(day_num)\n",
        "\n",
        "    day = {}\n",
        "    for ccy_pair in [(x, y) for x in range(NUM_CCYS) for y in range(NUM_CCYS) if x > y]:\n",
        "      day[ccy_pair] = []\n",
        "    for client in self.get_clients_active(day_num):\n",
        "      times, volumes = self.get_client_trades(day_num, client)\n",
        "      if client.buy_ccy > client.sell_ccy:\n",
        "        day[client.buy_ccy, client.sell_ccy] += zip(times, volumes)\n",
        "      else:\n",
        "        day[client.sell_ccy, client.buy_ccy] += zip(times, -1 * volumes)\n",
        "    return day\n",
        "\n",
        "  def next(self, day_num: int, use_seed: bool = False):\n",
        "    if use_seed:\n",
        "      np.random.seed(day_num)\n",
        "\n",
        "    day = np.zeros((DAY_LEN, NUM_CCYS), dtype=np.int32)\n",
        "    for client in self.get_clients_active(day_num):\n",
        "      times, volumes = self.get_client_trades(day_num, client)\n",
        "      for i in range(len(times)):\n",
        "        day[times[i], client.buy_ccy] -= volumes[i]\n",
        "        day[times[i], client.sell_ccy] += volumes[i]\n",
        "    return day\n",
        "\n",
        "day_generator = Day(get_clients())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JRphDptFT-P"
      },
      "source": [
        "if RUN:\n",
        "  average_day = day_generator.next(0)\n",
        "  for i in range(1,100):\n",
        "    average_day += day_generator.next(i)\n",
        "\n",
        "  average_day = average_day/100\n",
        "\n",
        "  plt.plot(average_day[:,0])\n",
        "  plt.xlabel('minute of day')\n",
        "  plt.ylabel('average postion change per minute')\n",
        "  plt.title(\"average postion change throughout day of currency 0\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.bar(list(range(NUM_CCYS)), average_day.sum(axis=0))\n",
        "  plt.xlabel('currency')\n",
        "  plt.ylabel('average net postion at end of day')\n",
        "  plt.title(\"currency average net postion at end of day\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SfgFLMknm2_"
      },
      "source": [
        "#Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjk0y6dbn2Bx"
      },
      "source": [
        "## Base Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAiHRZjVnqe0"
      },
      "source": [
        "from typing import Any, List, Sequence, Tuple\n",
        "\n",
        "class Environment:\n",
        "  def __init__(self, use_seed=False):\n",
        "    self.day_num = 0\n",
        "    self.beta = -1\n",
        "    self.alpha = -1\n",
        "    self.use_seed = use_seed\n",
        "    np.random.seed(1)\n",
        "\n",
        "  def reset(self):\n",
        "    self.day = day_generator.next(self.day_num, use_seed=self.use_seed)\n",
        "    self.min = 0\n",
        "    self.day_num += 1\n",
        "    self.position = np.zeros(NUM_CCYS, dtype=np.int32)\n",
        "    return self.get_state()\n",
        "  \n",
        "  def get_alpha_reward(self):\n",
        "    return np.array([self.alpha * np.absolute(self.position).sum(), 0], np.int32)\n",
        "\n",
        "  def get_beta_reward(self, amount_heged):\n",
        "    return np.array([0, self.beta * (amount_heged + 1)], np.int32)\n",
        "    \n",
        "  def get_bucket(self, pos: int) -> int:\n",
        "    return min(math.frexp(abs(pos)/SMALLEST_BUCKET)[1], NUM_BUCKETS - 1)\n",
        "\n",
        "  def amount_heged(self, ccy, bucket):\n",
        "    if bucket == 1:\n",
        "      return self.position[ccy]\n",
        "    else:\n",
        "      return self.position[ccy] // 2 if self.position[ccy] > 0 else (self.position[ccy] - 1) // 2\n",
        "  \n",
        "  def wait(self):\n",
        "    reward = self.get_alpha_reward()\n",
        "    self.position += self.day[self.min]\n",
        "    self.min += 1\n",
        "    return reward\n",
        "  \n",
        "  def hedge(self, ccy1, ccy2, bucket):\n",
        "    amount_heged = self.amount_heged(ccy1, bucket)\n",
        "    self.position[ccy1] -= amount_heged\n",
        "    self.position[ccy2] += amount_heged\n",
        "    return self.get_beta_reward(abs(amount_heged))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjbkWlETn8Tj"
      },
      "source": [
        "## Tabulular Environments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb9aX-Fqn0z7"
      },
      "source": [
        "@dataclass\n",
        "class Action:\n",
        "  wait: bool=False\n",
        "  ccy1: int=None\n",
        "  ccy2: int=None\n",
        "  bucket: int=None\n",
        "\n",
        "class EnvTabular(Environment):\n",
        "  def get_state(self):\n",
        "    bucketed_positions = list(map(lambda x: self.get_bucket(x) * (1 if x > 0 else -1), self.position))\n",
        "    return np.array(bucketed_positions + [self.min // LEN_TIME_PERIOD])\n",
        "\n",
        "  def imagine_step(self, action: Action) -> np.ndarray:\n",
        "    state = None\n",
        "    if action.wait:\n",
        "      state = self.get_state() \n",
        "    else:\n",
        "      position_ccy1 = self.position[action.ccy1]\n",
        "      position_ccy2 = self.position[action.ccy2]\n",
        "      amount_heged = self.amount_heged(action.ccy1, action.bucket)\n",
        "      self.position[action.ccy1] -= amount_heged\n",
        "      self.position[action.ccy2] += amount_heged\n",
        "      state = self.get_state()\n",
        "      self.position[action.ccy1] = position_ccy1\n",
        "      self.position[action.ccy2] = position_ccy2\n",
        "    return state\n",
        "\n",
        "  def step(self, action: Action) -> Tuple[np.ndarray, np.ndarray, bool]:\n",
        "    reward = None\n",
        "    if action.wait:\n",
        "      reward = self.wait()\n",
        "    else:\n",
        "      reward = self.hedge(action.ccy1, action.ccy2, action.bucket)\n",
        "    return (self.get_state(), reward, self.min >= DAY_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF81JApXnx4g"
      },
      "source": [
        "class EnvTabularParallel(EnvTabular):\n",
        "  def get_alpha_reward(self):\n",
        "    return np.append(self.alpha * np.absolute(self.position).astype(np.float32), 0)\n",
        "\n",
        "  def get_beta_reward(self, amount_heged):\n",
        "    return np.array(([0] * NUM_CCYS) + [self.beta * (amount_heged + 1)], np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvlSRF6ioB-5"
      },
      "source": [
        "## Neural Net Environments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arBWisUenwVZ"
      },
      "source": [
        "class EnvNN(Environment):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.action_space =[2, NUM_CCYS, NUM_CCYS, ACTION_BUCKETS] \n",
        "    self.state_space = NUM_CCYS * NUM_BUCKETS * 2 + NUM_TIME_PERIODS\n",
        "\n",
        "  def encode_position(self, ccy):\n",
        "    start = ccy * 2 * NUM_BUCKETS \n",
        "    return [start + (0 if self.position[ccy] < 0 else 1), start + self.get_bucket(self.position[ccy]) * 2 \\\n",
        "                 + (1 if self.position[ccy] < 0 else 0)] \n",
        "\n",
        "  def print_state(self):\n",
        "    state = self.get_state()\n",
        "    print(state[:NUM_CCYS * NUM_BUCKETS * 2].reshape((NUM_CCYS, NUM_BUCKETS * 2)))\n",
        "    print(state[NUM_CCYS * NUM_BUCKETS * 2:])\n",
        "\n",
        "  def get_state(self):\n",
        "    state = np.zeros(self.state_space, dtype=np.int32)\n",
        "    for ccy in range(NUM_CCYS):\n",
        "      state[self.encode_position(ccy)] = 1\n",
        "    state[-(self.min // LEN_TIME_PERIOD)] = 1\n",
        "    return state\n",
        "\n",
        "  def hedge(self, ccy1, ccy2, bucket_size):\n",
        "    if self.position[ccy1] <= 0 or self.position[ccy2] >= 0:\n",
        "      return -np.ones(2)\n",
        "    if abs(self.position[ccy1]) < abs(self.position[ccy2]):\n",
        "      return super().hedge(ccy1, ccy2, bucket_size)\n",
        "    else:\n",
        "      return super().hedge(ccy2, ccy1, bucket_size)  \n",
        "\n",
        "  def step(self, action):\n",
        "    if action[0] == 0:\n",
        "      reward = np.zeros(2, np.int32)\n",
        "    else:\n",
        "      reward = self.hedge(action[1], action[2], action[3])\n",
        "    reward += self.wait()\n",
        "    return self.get_state(), reward, np.int64(self.min >= DAY_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdhcQyMFER92"
      },
      "source": [
        "class EnvNNShort(EnvNN):  \n",
        "  def step(self, action):\n",
        "    if action[0] == 0:\n",
        "      reward = np.zeros(2, np.int32)\n",
        "    else:\n",
        "      reward = self.hedge(action[1], action[2], action[3])\n",
        "    for i in range(10):\n",
        "      reward += self.wait()\n",
        "    return self.get_state(), reward, np.int64(self.min >= DAY_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzawZqRBxhPB"
      },
      "source": [
        "#Algorithmic solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXJQhdSDhFA"
      },
      "source": [
        "A simplistic algorithic algorithem to be used for benchmarking I have dubed X minute hedging.\n",
        "\n",
        "The algorithem has not been implimented, just the calculations to get the reward.\n",
        "\n",
        "The algorithm is to keep track of the net position change between every single pair of currencies and neutralize the position every X minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIR8_8KWD_5w"
      },
      "source": [
        "import math\n",
        "\n",
        "def x_minute_hedging(between_hedging_time: int = 5, number_of_days=5):\n",
        "  np.seed = 0\n",
        "  reward = np.zeros(2)\n",
        "  for day_num in range(number_of_days):\n",
        "    day = day_generator.next(day_num=day_num, use_seed=True)\n",
        "    for ccys, ccy_pair_trades in day_generator.next_clients(day_num=day_num, use_seed=True).items():\n",
        "      buckets = np.zeros(DAY_LEN // between_hedging_time)\n",
        "      for trade in ccy_pair_trades:\n",
        "        buckets[min(len(buckets) -1 ,trade[0] // between_hedging_time)] += trade[1]\n",
        "      for bucket_num, bucket in enumerate(buckets):\n",
        "        trade_min = min(DAY_LEN - 1, (bucket_num + 1) * between_hedging_time - 1)\n",
        "        day[trade_min, ccys[0]] += bucket\n",
        "        day[trade_min, ccys[1]] -= bucket\n",
        "        if bucket != 0:\n",
        "          reward[1] -= abs(bucket) + 1\n",
        "    \n",
        "    pos = np.zeros(NUM_CCYS)\n",
        "    for minute in range(DAY_LEN):\n",
        "      pos += day[minute]\n",
        "      reward[0] -= np.sum(np.absolute(pos))\n",
        "  scaler = 1 / number_of_days\n",
        "  return (scaler * reward).round(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNtOIvRwFX4c"
      },
      "source": [
        "if RUN:\n",
        "  print(\"alpha bata rewards calculated using the x minute hedging algorithem for a day :\")\n",
        "  print(\"only hedging once a day \" + str(x_minute_hedging(between_hedging_time=1440)))\n",
        "  print(\"hedging every hour \" + str(x_minute_hedging(between_hedging_time=60)))\n",
        "  print(\"hedging every 20 minutes \" + str(x_minute_hedging(between_hedging_time=20)))\n",
        "  print(\"hedging every 5 minutes \" + str(x_minute_hedging(between_hedging_time=5)))\n",
        "  print(\"hedging every minute \" + str(x_minute_hedging(between_hedging_time=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiHwuhhha5IG"
      },
      "source": [
        "if RUN:\n",
        "  times = [1440, 720, 360, 180, 90, 45, 22, 11, 5, 2, 1]\n",
        "  results = []\n",
        "  for time in times:\n",
        "    results.append(-x_minute_hedging(between_hedging_time=time))\n",
        "  results = np.array(results)\n",
        "\n",
        "  plt.rcParams[\"figure.figsize\"] = [10,6]\n",
        "  plt.plot(times, results[:, 0], label=\"alpha\")\n",
        "  plt.plot(times, results[:, 1], label=\"beta\")\n",
        "  plt.yscale('log')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Minute between hedgings')\n",
        "  plt.ylabel('Negitive reward')\n",
        "  plt.title(\"Rewards for hedging every x minutes\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffb369pQyTo3"
      },
      "source": [
        "def neutralize_pos_triangular(env, state):\n",
        "  def smallest_nonzero(state: np.ndarray) -> int:\n",
        "    valid_idx = np.where(state != 0)[0]\n",
        "    smallest = None\n",
        "    if valid_idx.size != 0:\n",
        "      smallest = valid_idx[np.argmin(np.absolute(state[valid_idx]))]\n",
        "    return smallest\n",
        "\n",
        "  def hedge_against(state: list, idx: int) -> int:\n",
        "    chosen_ccy = None\n",
        "    if idx != None:\n",
        "      counter_ccys = np.where(state < 0 if state[idx] > 0 else state > 0)[0]\n",
        "      if counter_ccys.size != 0:\n",
        "        chosen_ccy = counter_ccys[smallest_nonzero(state[counter_ccys])]\n",
        "    return chosen_ccy\n",
        "\n",
        "  reward_sum = np.zeros(2)\n",
        "  ccy1 = smallest_nonzero(state)\n",
        "  ccy2 = hedge_against(state, ccy1)\n",
        "  while ccy2 != None:\n",
        "    state, reward, finished = env.step(Action(ccy1=ccy1, ccy2=ccy2, bucket=1))\n",
        "    state[-1] = 0\n",
        "    reward_sum += reward\n",
        "    ccy1 = smallest_nonzero(state)\n",
        "    ccy2 = hedge_against(state, ccy1)\n",
        "  return reward_sum\n",
        "\n",
        "def x_minute_hedging_triangular(between_hedging_time: int = 5, number_of_days=5):\n",
        "  np.seed = 0\n",
        "  reward_sum = np.zeros(2)\n",
        "  env = EnvTabular(use_seed=True)\n",
        "  for _ in range(number_of_days):\n",
        "    state = env.reset()\n",
        "    finished = False\n",
        "    while not(finished):\n",
        "      for _ in range(between_hedging_time):\n",
        "        state, reward, finished = env.step(Action(wait=True))\n",
        "        state[-1] = 0\n",
        "        reward_sum += reward\n",
        "        if finished:\n",
        "          break\n",
        "      reward_sum += neutralize_pos_triangular(env, state)\n",
        "\n",
        "  scaler = 1 / number_of_days\n",
        "  return (scaler * reward_sum).round(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv7J2sDYeS-7"
      },
      "source": [
        "if RUN:\n",
        "  print(\"alpha bata rewards calculated using the x triangular minute hedging algorithem for a day :\")\n",
        "  print(\"only hedging once a day \" + str(x_minute_hedging_triangular(between_hedging_time=1440)))\n",
        "  print(\"hedging every hour \" + str(x_minute_hedging_triangular(between_hedging_time=60)))\n",
        "  print(\"hedging every 20 minutes \" + str(x_minute_hedging_triangular(between_hedging_time=20)))\n",
        "  print(\"hedging every 5 minutes \" + str(x_minute_hedging_triangular(between_hedging_time=5)))\n",
        "  print(\"hedging every minute \" + str(x_minute_hedging_triangular(between_hedging_time=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOYYcMqD4KsN"
      },
      "source": [
        "if RUN:\n",
        "  times = [1440, 720, 360, 180, 90, 45, 22, 11, 5, 2, 1]\n",
        "  results = []\n",
        "  for time in times:\n",
        "    results.append(-x_minute_hedging_triangular(between_hedging_time=time))\n",
        "  results = np.array(results)\n",
        "\n",
        "  plt.rcParams[\"figure.figsize\"] = [10,6]\n",
        "  plt.plot(times, results[:, 0], label=\"alpha\")\n",
        "  plt.plot(times, results[:, 1], label=\"beta\")\n",
        "  plt.yscale('log')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Minute between hedgings')\n",
        "  plt.ylabel('Negitive reward')\n",
        "  plt.title(\"Rewards for hedging triangularly every x minutes\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKYw9j7o59UX"
      },
      "source": [
        "if RUN:\n",
        "  times = [1440, 720, 360, 180, 90, 45, 22, 11, 5, 2, 1]\n",
        "  results = []\n",
        "  results1 = []\n",
        "  for time in times:\n",
        "    results.append(-x_minute_hedging_triangular(between_hedging_time=time))\n",
        "    results1.append(-x_minute_hedging(between_hedging_time=time))\n",
        "  results = np.array(results)\n",
        "  results1 = np.array(results1)\n",
        "\n",
        "  plt.rcParams[\"figure.figsize\"] = [10,6]\n",
        "  plt.plot(times, results[:, 1], label=\"x minute triangular\")\n",
        "  plt.plot(times, results1[:, 1], label=\"standard x minute\")\n",
        "  plt.yscale('log')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Minute between hedgings')\n",
        "  plt.ylabel('Negitive beta reward')\n",
        "  plt.title(\"Comparison of beta rewards for simple hedging algorithems\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_24ugQxc8R9"
      },
      "source": [
        "#Tabular learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NOYUt09UXVt"
      },
      "source": [
        "## Base Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhL7y2lC3zPx"
      },
      "source": [
        "class TabularLearning:\n",
        "  def set_variables(self):\n",
        "    self.alpha = 0.5 # leaning rate\n",
        "    self.epsilon = 0.5 # explore rate\n",
        "    self.step_foward_randomly = 0.9 # the chance that when selecting a random action, that action will be to wait\n",
        "    self.epsilon_rate = 0.999 # rate at which alpha and epsilon aproach 0\n",
        "    self.alpha_rate = 0.9996 # rate at which alpha and epsilon aproach 0\n",
        "\n",
        "  def get_nonzero_ccys(self, state) -> list:\n",
        "    return [i for i in range(NUM_CCYS) if state[i] != 0]\n",
        "\n",
        "  def update_alpha_epsilon(self):\n",
        "    self.alpha = self.alpha * self.alpha_rate\n",
        "    self.epsilon = self.epsilon * self.epsilon_rate\n",
        "\n",
        "  def update_function(self, state_value, new_state_value, reward):\n",
        "    return state_value + self.alpha * (reward + new_state_value - state_value)\n",
        "    \n",
        "  def get_random_move(self, state):\n",
        "      nonzero_ccys = self.get_nonzero_ccys(state)\n",
        "      if np.random.random() < self.step_foward_randomly or len(nonzero_ccys) <= 1:\n",
        "        action = Action(wait=True)\n",
        "      else:\n",
        "        ccy1 = np.random.choice(nonzero_ccys)\n",
        "        ccy2 = np.random.choice([i for i in nonzero_ccys if (state[i] > 0 and state[ccy1] < 0) or (state[i] < 0 and state[ccy1] > 0)])\n",
        "        action = Action(ccy1=ccy1, ccy2=ccy2, bucket=np.random.choice([0, 1]))\n",
        "      return action\n",
        "\n",
        "  def train_step(self, state):\n",
        "    if np.random.random() < self.epsilon:\n",
        "      action = self.get_random_move(state)\n",
        "    else:\n",
        "      action = self.best_action(state)\n",
        "    old_state = state\n",
        "    state, reward, finished = self.env.step(action)\n",
        "    self.update_table(old_state, state, action, reward, finished)\n",
        "    return state, reward, finished \n",
        "\n",
        "  def learn(self, num_days):\n",
        "    day_reward = np.zeros((num_days,2))\n",
        "    for i in range(num_days):\n",
        "      state = self.env.reset()\n",
        "      finished = False\n",
        "      while not(finished):\n",
        "        state, reward, finished = self.train_step(state)\n",
        "        day_reward[i] += (np.sum(reward[:-1]), reward[-1])\n",
        "      self.update_alpha_epsilon()\n",
        "      if i % 100 == 0:\n",
        "        print(\"rewards \" + str(day_reward[i]/1000)  + \", sum rewards \" + str(np.sum(day_reward[i])/1000) + \", alpha \"+ str((self.alpha)))\n",
        "    return day_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDW7o1ovRAgg"
      },
      "source": [
        "## Q Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_vJaXJnEVc8"
      },
      "source": [
        "class QLearning(TabularLearning): \n",
        "  def __init__(self):\n",
        "    self.q_table = {}\n",
        "    self.env = EnvTabular()\n",
        "    self.set_variables()\n",
        "\n",
        "  def get_q(self, key):\n",
        "    return self.q_table.get(key, 0)\n",
        "\n",
        "  def encode_state(self, state, action):\n",
        "    return str((state, action))\n",
        "\n",
        "  def get_encoded_q(self, state, action):\n",
        "    return self.get_q(self.encode_state(state, action))\n",
        "\n",
        "  def update_table(self, old_state, new_state, action, reward, finished):\n",
        "    old_state_value = self.get_encoded_q(old_state, action)\n",
        "    new_state_value = self.get_encoded_q(new_state, self.best_action(new_state))\n",
        "    self.q_table[self.encode_state(old_state, action)] = self.update_function(old_state_value, new_state_value, sum(reward))\n",
        "    \n",
        "  def best_action(self, state):\n",
        "    best_action = Action(wait=True)\n",
        "    best_action_value = self.get_encoded_q(state, best_action)\n",
        "    nonzero_ccys = self.get_nonzero_ccys(state)\n",
        "    for i in nonzero_ccys:\n",
        "      for j in [j for j in nonzero_ccys if (state[i] > 0 and state[j] < 0) or (state[i] < 0 and state[j] > 0)]:\n",
        "        for bucket in [0, 1]:\n",
        "          action = Action(ccy1=i, ccy2=j, bucket=bucket)\n",
        "          action_value = self.get_encoded_q(state, action)\n",
        "          if action_value > best_action_value:\n",
        "            best_action_value = action_value\n",
        "            best_action = action\n",
        "    return best_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLOLxEHdFEF9"
      },
      "source": [
        "## V Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNqqEJ9iuheY"
      },
      "source": [
        "class VLearning(TabularLearning): \n",
        "  def __init__(self):\n",
        "    self.env = EnvTabular()\n",
        "    self.v_table = {}\n",
        "    self.set_variables()\n",
        "\n",
        "  def get_v(self, key):\n",
        "    return self.v_table.get(key, 0)\n",
        "\n",
        "  def encode_state(self, state):\n",
        "    return str(state) \n",
        "\n",
        "  def encode_wait_state(self, state):\n",
        "    return str((state, \"w\"))\n",
        "\n",
        "  def hedge_reward(self, action):\n",
        "    return self.env.get_beta_reward(abs(self.env.amount_heged(action.ccy1, action.bucket)))[-1]\n",
        "\n",
        "  def update_table(self, old_state, new_state, action, reward, finished):\n",
        "    state_value = self.get_v(self.encode_state(old_state))\n",
        "    new_state_value = self.get_v(self.encode_state(new_state))\n",
        "    self.v_table[self.encode_state(old_state)] = self.update_function(state_value, new_state_value, sum(reward))\n",
        "    if action.wait:\n",
        "      state_value = self.get_v(self.encode_wait_state(old_state))\n",
        "      self.v_table[self.encode_wait_state(old_state)] = self.update_function(state_value, new_state_value, sum(reward))\n",
        "\n",
        "  def best_action(self, state):\n",
        "    best_action = Action(wait=True)\n",
        "    best_action_value = self.get_v(self.encode_wait_state(state))\n",
        "    nonzero_ccys = self.get_nonzero_ccys(state)\n",
        "    for i in nonzero_ccys:\n",
        "      for j in [j for j in nonzero_ccys if (state[i] > 0 and state[j] < 0) or (state[i] < 0 and state[j] > 0)]:\n",
        "        for bucket in [0, 1]:\n",
        "          action = Action(ccy1=i, ccy2=j, bucket=bucket)\n",
        "          action_value = self.get_v(self.encode_state(self.env.imagine_step(action))) + self.hedge_reward(action)\n",
        "          if action_value > best_action_value:\n",
        "            best_action_value = action_value\n",
        "            best_action = action\n",
        "    return best_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QrmnkBy1E2f"
      },
      "source": [
        "## V Learning Differentiated State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHDc3aKU9TUP"
      },
      "source": [
        "class VLearningParallel(VLearning):\n",
        "  def __init__(self):\n",
        "    self.env = EnvTabularParallel()\n",
        "    self.v_table = {}\n",
        "    self.set_variables()\n",
        "\n",
        "  def get_v_singular(self, state):\n",
        "    return super().get_v(state)\n",
        "\n",
        "  def get_v(self, state):\n",
        "    return sum(map(lambda x: self.get_v_singular(x), state))\n",
        "    \n",
        "  def encode_state(self, state):\n",
        "    return [str((i, state[i], state[-1])) for i in range(NUM_CCYS)]\n",
        "\n",
        "  def encode_wait_state(self, state):\n",
        "    return [str((i, state[i], state[-1], \"w\")) for i in range(NUM_CCYS)]\n",
        "\n",
        "  def update_table(self, old_state, new_state, action, reward, finished):\n",
        "    old_state_encoded = self.encode_state(old_state)\n",
        "    new_state_encoded = self.encode_state(new_state)\n",
        "    if action.wait:\n",
        "      old_state_wait_encoded = self.encode_wait_state(old_state)\n",
        "      for i in range(NUM_CCYS):\n",
        "        state_value = self.get_v_singular(old_state_encoded[i])\n",
        "        new_state_value = self.get_v_singular(new_state_encoded[i])\n",
        "        self.v_table[old_state_encoded[i]] = self.update_function(state_value, new_state_value, reward[i])\n",
        "        state_value = self.get_v_singular(old_state_wait_encoded[i])\n",
        "        self.v_table[old_state_wait_encoded[i]] = self.update_function(state_value, new_state_value, reward[i])\n",
        "    else:\n",
        "      for i in [action.ccy1, action.ccy2]:\n",
        "        state_value = self.get_v_singular(old_state_encoded[i])\n",
        "        new_state_value = self.get_v_singular(new_state_encoded[i])\n",
        "        self.v_table[old_state_encoded[i]] = self.update_function(state_value, new_state_value, reward[-1]/2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deLdDU3X9aUG"
      },
      "source": [
        "## N-step V learning differentiated state space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rucz3uRO9XRu"
      },
      "source": [
        "class NstepVLearningParallel(VLearningParallel):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.n = 2\n",
        "    self.n_pointer = 0\n",
        "    self.cached_rewards = np.empty(self.n, dtype=np.object)\n",
        "    self.cached_state_actions = np.empty(self.n, dtype=np.object)\n",
        "\n",
        "  def update_table(self, old_state, new_state, action, reward, finished):\n",
        "    if self.cached_state_actions[-1] == None:\n",
        "      self.cached_state_actions[self.n_pointer] = (old_state, action)\n",
        "      self.cached_rewards[self.n_pointer] = reward\n",
        "      self.n_pointer += 1\n",
        "    else:\n",
        "      old_action = self.cached_state_actions[0][1]\n",
        "      if old_action.wait:\n",
        "        super().update_table(\n",
        "              old_state=self.cached_state_actions[0][0],\n",
        "              new_state=new_state,\n",
        "              action=old_action, \n",
        "              reward=self.cached_rewards.sum(axis=0),\n",
        "              finished=finished\n",
        "            )\n",
        "      else:\n",
        "        super().update_table(\n",
        "              old_state=self.cached_state_actions[0][0],\n",
        "              new_state=self.cached_state_actions[1][0],\n",
        "              action=old_action, \n",
        "              reward=self.cached_rewards[0],\n",
        "              finished=finished\n",
        "            )\n",
        "      self.cached_state_actions[0] = (old_state, action)\n",
        "      if not action.wait:\n",
        "        reward[action.ccy1] += reward[-1]/2\n",
        "        reward[action.ccy2] += reward[-1]/2\n",
        "      self.cached_rewards[0] = reward\n",
        "      self.cached_state_actions = np.roll(self.cached_state_actions, -1)\n",
        "      self.cached_rewards = np.roll(self.cached_rewards, -1)\n",
        "      if finished:\n",
        "        zero_rewards = np.zeros_like(reward)\n",
        "        for i in range(self.n - 1):\n",
        "          self.update_table(\n",
        "              old_state=new_state,\n",
        "              new_state=new_state,\n",
        "              action=Action(wait=True), \n",
        "              reward=zero_rewards,\n",
        "              finished=False\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUqRLBTATCPH"
      },
      "source": [
        "#A2C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmUt_MRKuq0J"
      },
      "source": [
        "import sys\n",
        "import torch  \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "  def __init__(self, num_inputs, num_actions_list):\n",
        "    super(ActorCritic, self).__init__()\n",
        "    self.shared_layers = nn.Sequential(\n",
        "          nn.Linear(num_inputs, 1024),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(1024, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 256),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "    self.critic_linear = nn.Linear(256, 1)\n",
        "    self.actors_linear = nn.ModuleList([nn.Linear(256, num_actions) for num_actions in num_actions_list])\n",
        "    \n",
        "  def forward(self, state):\n",
        "    shared_nodes_value = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
        "    shared_nodes_value = self.shared_layers(shared_nodes_value)\n",
        "    value = self.critic_linear(shared_nodes_value)\n",
        "    policy_dists = []\n",
        "    for actor_layer in self.actors_linear:\n",
        "      policy_dists.append(F.softmax(F.relu(actor_layer(shared_nodes_value)), dim=1))\n",
        "\n",
        "    return policy_dists, value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uB3pPlA4SpO"
      },
      "source": [
        "class A2C():\n",
        "  def __init__(self):\n",
        "    self.env = EnvNN() # MyCartPole2()\n",
        "    self.actor_critic = ActorCritic(self.env.state_space, self.env.action_space)\n",
        "    self.ac_optimizer = optim.Adam(self.actor_critic.parameters(), lr=1e-4)\n",
        "    self.eps = np.finfo(np.float32).eps.item()\n",
        "    \n",
        "  def learn(self, num_days):\n",
        "    day_reward = np.zeros((num_days,2))\n",
        "    self.entropy_term = 0\n",
        "    for i in range(num_days):\n",
        "      reward = self.train_episode(batch_size=np.random.randint(5,50))\n",
        "      day_reward[i] += (np.sum(reward[:-1]), reward[-1])\n",
        "      if i % 100 == 0:\n",
        "        print(\"rewards \" + str(day_reward[i]/1000)  + \", sum rewards \" + str(np.sum(day_reward[i])/1000))\n",
        "    return day_reward\n",
        "\n",
        "  def train_episode(self, n_step=50, batch_size=10):\n",
        "    tot_rewards = np.zeros(2)\n",
        "    log_probs = []\n",
        "    values = []\n",
        "    rewards = []\n",
        "    done = False\n",
        "    state = self.env.reset()\n",
        "    step = 0\n",
        "    \n",
        "    while not done:\n",
        "      policy_dist, value = self.actor_critic.forward(state)\n",
        "      values.append(value)\n",
        "      dists = list(map(lambda x: x.detach().numpy() + self.eps, policy_dist))\n",
        "      actions = list(map(lambda dist: np.random.choice(dist.shape[1], p=np.squeeze(dist)), dists))\n",
        "      self.entropy_term += sum(map(lambda dist: np.sum(np.mean(dist) * np.log(dist)), dists))\n",
        "      if actions[0] == 1:\n",
        "        log_probs.append(torch.log(policy_dist[0].squeeze(0)[actions[0]]))\n",
        "      else:\n",
        "        log_probs.append(sum(map(lambda i: torch.log(policy_dist[i].squeeze(0)[actions[i]]), range(len(actions)))))\n",
        "      state, reward, done = self.env.step(actions)\n",
        "      tot_rewards += reward\n",
        "      rewards.append(reward.sum())\n",
        "      step += 1\n",
        "      \n",
        "      if step % (batch_size + n_step) == 0 or done:\n",
        "        if done and step % (batch_size + n_step) != 0:\n",
        "          final_update_size = step % (batch_size + n_step)\n",
        "          if final_update_size < batch_size:\n",
        "            batch_size = final_update_size\n",
        "          rewards += [0 for _ in range(batch_size + n_step - len(rewards))]\n",
        "          values += [0 for _ in range(batch_size + n_step - len(values))]\n",
        "        \n",
        "        batch_rewards = torch.FloatTensor([sum(rewards[t: t + n_step]) for t in range(batch_size)])\n",
        "        batch_values_t = torch.stack(values[: batch_size])\n",
        "        batch_values_t_plus_n = torch.FloatTensor(values[n_step: batch_size + n_step])\n",
        "        log_probs = torch.stack(log_probs[: batch_size])\n",
        "\n",
        "        advantage = batch_rewards - batch_values_t + batch_values_t_plus_n\n",
        "        actor_loss = (advantage * log_probs).mean()\n",
        "        critic_loss = 0.2 * (advantage ** 2).mean()\n",
        "        ac_loss = -actor_loss + critic_loss + 0.0001 * self.entropy_term\n",
        "\n",
        "        self.ac_optimizer.zero_grad()        \n",
        "        ac_loss.backward()\n",
        "        self.ac_optimizer.step()\n",
        "\n",
        "        rewards = []\n",
        "        values = []\n",
        "        log_probs = []\n",
        "        \n",
        "    return tot_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNpm06ZIhiYv"
      },
      "source": [
        "#A2C Single Actor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIfx3EV-hiY2"
      },
      "source": [
        "class A2CSingleActor():\n",
        "  def __init__(self):\n",
        "    self.env = EnvNN()\n",
        "    self.actor_critic = ActorCritic(self.env.state_space, [1 + np.prod(self.env.action_space)//2])\n",
        "    self.ac_optimizer = optim.Adam(self.actor_critic.parameters(), lr=1e-4)\n",
        "    self.eps = np.finfo(np.float32).eps.item()\n",
        "    \n",
        "  def learn(self, num_days):\n",
        "    day_reward = np.zeros((num_days,2))\n",
        "    self.entropy_term = 0\n",
        "    for i in range(num_days):\n",
        "      reward = self.train_episode(batch_size=np.random.randint(5, 25))\n",
        "      day_reward[i] += (np.sum(reward[:-1]), reward[-1])\n",
        "      if i % 100 == 0:\n",
        "        print(\"rewards \" + str(day_reward[i]/1000)  + \", sum rewards \" + str(np.sum(day_reward[i])/1000))\n",
        "    return day_reward\n",
        "\n",
        "  def action_parser(self, action):\n",
        "    if action == 0:\n",
        "      actions = [0]\n",
        "    else:\n",
        "      action -= 1\n",
        "      actions = [1]\n",
        "      for max_action in self.env.action_space[1:]:\n",
        "        actions.append(action % max_action)\n",
        "        action = action // max_action\n",
        "    return actions\n",
        "\n",
        "  def train_episode(self, n_step=15, batch_size=10):\n",
        "    tot_rewards = np.zeros(2)\n",
        "    log_probs = []\n",
        "    values = []\n",
        "    rewards = []\n",
        "    done = False\n",
        "    state = self.env.reset()\n",
        "    step = 0\n",
        "    \n",
        "    while not done:\n",
        "      policy_dist, value = self.actor_critic.forward(state)\n",
        "      values.append(value)\n",
        "      dist = policy_dist[0].detach().numpy() + self.eps\n",
        "      action = np.random.choice(dist.shape[1], p=np.squeeze(dist))\n",
        "      self.entropy_term += np.sum(np.mean(dist) * np.log(dist))\n",
        "      log_probs.append(torch.log(policy_dist[0].squeeze(0)[action]))\n",
        "      state, reward, done = self.env.step(self.action_parser(action))\n",
        "      tot_rewards += reward\n",
        "      rewards.append(reward.sum())\n",
        "      step += 1\n",
        "      \n",
        "      if step % (batch_size + n_step) == 0 or done:\n",
        "        if done and step % (batch_size + n_step) != 0:\n",
        "          final_update_size = step % (batch_size + n_step)\n",
        "          if final_update_size < batch_size:\n",
        "            batch_size = final_update_size\n",
        "          rewards += [0 for _ in range(batch_size + n_step - len(rewards))]\n",
        "          values += [0 for _ in range(batch_size + n_step - len(values))]\n",
        "        \n",
        "        batch_rewards = torch.FloatTensor([sum(rewards[t: t + n_step]) for t in range(batch_size)])\n",
        "        batch_values_t = torch.stack(values[: batch_size])\n",
        "        batch_values_t_plus_n = torch.FloatTensor(values[n_step: batch_size + n_step])\n",
        "        log_probs = torch.stack(log_probs[: batch_size])\n",
        "\n",
        "        advantage = batch_rewards - batch_values_t + batch_values_t_plus_n\n",
        "        actor_loss = (advantage * log_probs).mean()\n",
        "        critic_loss = 0.2 * (advantage ** 2).mean()\n",
        "        ac_loss = -actor_loss + critic_loss + 0.0001 * self.entropy_term\n",
        "\n",
        "        self.ac_optimizer.zero_grad()        \n",
        "        ac_loss.backward()\n",
        "        self.ac_optimizer.step()\n",
        "\n",
        "        rewards = []\n",
        "        values = []\n",
        "        log_probs = []\n",
        "        \n",
        "    return tot_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZNJPOsbucZd"
      },
      "source": [
        "#Reinforce"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k2VJe8vjYzj"
      },
      "source": [
        "class ReinforceNetwork(nn.Module):\n",
        "  def __init__(self, num_inputs, num_actions_list):\n",
        "    super(ReinforceNetwork, self).__init__()\n",
        "    self.shared_layers = nn.Sequential(\n",
        "          nn.Linear(num_inputs, 1024),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(1024, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 256),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "    self.actors_linear = nn.ModuleList([nn.Linear(256, num_actions) for num_actions in num_actions_list])\n",
        "    \n",
        "  def forward(self, state):\n",
        "    shared_nodes_value = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
        "    shared_nodes_value = self.shared_layers(shared_nodes_value)\n",
        "    policy_dists = []\n",
        "    for actor_layer in self.actors_linear:\n",
        "      policy_dists.append(F.softmax(F.relu(actor_layer(shared_nodes_value)), dim=1))\n",
        "    return policy_dists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNjpjarnhf4w"
      },
      "source": [
        "class Reinforce():\n",
        "  def __init__(self):\n",
        "    self.env = EnvNNShort() #MyCartPole2()\n",
        "    self.network = ReinforceNetwork(self.env.state_space, self.env.action_space) \n",
        "    self.ac_optimizer = optim.Adam(self.network.parameters(), lr=1e-4)\n",
        "    self.eps   = np.finfo(np.float32).eps.item()\n",
        "\n",
        "  def learn(self, num_days):\n",
        "    day_reward = np.zeros((num_days,2))\n",
        "    self.entropy_term = 0\n",
        "    for i in range(num_days):\n",
        "      reward = self.train_episode()\n",
        "      day_reward[i] += (np.sum(reward[:-1]), reward[-1])\n",
        "      if i % 100 == 0:\n",
        "        print(\"rewards \" + str(day_reward[i]/1000)  + \", sum rewards \" + str(np.sum(day_reward[i])/1000))\n",
        "    return day_reward\n",
        "\n",
        "  def train_episode(self):\n",
        "    tot_rewards = np.zeros(2)\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    done = False\n",
        "    state = self.env.reset()\n",
        "\n",
        "    while not done:\n",
        "      policy_dist = self.network.forward(state)\n",
        "      dists = list(map(lambda x: x.detach().numpy() + self.eps, policy_dist))\n",
        "      actions = list(map(lambda dist: np.random.choice(dist.shape[1], p=np.squeeze(dist)), dists))\n",
        "      self.entropy_term += sum(map(lambda dist: np.sum(np.mean(dist) * np.log(dist)), dists))\n",
        "      if actions[0] == 1:\n",
        "        log_probs.append(torch.log(policy_dist[0].squeeze(0)[actions[0]]))\n",
        "      else:\n",
        "        log_probs.append(sum(map(lambda i: torch.log(policy_dist[i].squeeze(0)[actions[i]]), range(len(actions)))))\n",
        "      state, reward, done = self.env.step(actions)\n",
        "      tot_rewards += reward\n",
        "      rewards.append(reward.sum())\n",
        "\n",
        "    Qval = 0\n",
        "    Qvals = np.zeros(len(rewards))\n",
        "    for t in reversed(range(len(rewards))):\n",
        "      Qval = rewards[t] + Qval\n",
        "      Qvals[t] = Qval\n",
        "\n",
        "    Qvals = torch.FloatTensor(Qvals)\n",
        "    log_probs = torch.stack(log_probs)\n",
        "\n",
        "    actor_loss = (-log_probs * Qvals).mean() + 0.0001 * self.entropy_term\n",
        "\n",
        "    self.ac_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    self.ac_optimizer.step()\n",
        "\n",
        "    return tot_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTKDrfH5lEUz"
      },
      "source": [
        "# Testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfUHT-jHltlX"
      },
      "source": [
        "## Admin\n",
        "\n",
        "Code to load and save models and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ7xNXHQmNYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2194bd2-8cc9-468e-926b-9c14553e7f10"
      },
      "source": [
        "if COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx89yASLpJ3l"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from skimage.measure import block_reduce\n",
        "\n",
        "def gen_path(file_name, beta_scale=1):\n",
        "  return os.path.dirname(os.path.realpath(\"__file__\")).replace(\"\\\\\", \"/\") + \\\n",
        "   (\"/gdrive/MyDrive/Colab/\" if COLAB else \"/\") + file_name + \"-beta-\" + str(beta_scale)\n",
        "\n",
        "def load_rewards(file_name, beta_scale, average_over=20):\n",
        "  directory_path = gen_path(file_name, beta_scale)\n",
        "  if not os.path.exists(directory_path):\n",
        "    print(\"no such directory : \" + directory_path)\n",
        "  rewards = np.loadtxt(directory_path + \"/rewards.txt\")\n",
        "  return block_reduce(rewards, block_size=(average_over,1), func=np.mean)\n",
        "\n",
        "def graph_results(filename=\"AI-Models/Q-learning-Parallel\", beta=2, x_min_comp=None, title=\"V-learning\", average=20): \n",
        "  rewards = load_rewards(filename, beta, average_over=average) * -1\n",
        "  plt.plot(rewards[:,0], label='alpha')\n",
        "  plt.plot(rewards[:,1], label='beta')\n",
        "\n",
        "  if x_min_comp != None:\n",
        "    comparison = x_minute_hedging_triangular(between_hedging_time=x_min_comp, number_of_days=x_min_comp) * -1\n",
        "    plt.plot(np.full((rewards.shape[0], 1), comparison[0]), label=\"algo alpha\")\n",
        "    plt.plot(np.full((rewards.shape[0], 1), comparison[1]), label=\"algo beta\")\n",
        "\n",
        "  plt.rcParams[\"figure.figsize\"] = [10,6]\n",
        "  plt.yscale('log')\n",
        "  plt.legend()\n",
        "  plt.ylabel('reward per episode')\n",
        "  plt.xlabel(f'{average} episode average')\n",
        "  plt.title(f'Rewards for {title},  beta scale {beta}')\n",
        "  plt.savefig(gen_path(filename, beta) + '/graph.png', bbox_inches='tight')\n",
        "  print(gen_path(filename, beta) + '/graph.png')\n",
        "\n",
        "def graph_results_multi(filenames, title=\"V-learning\", average=20):\n",
        "  for filename in filenames:\n",
        "    rewards = load_rewards(filename, 25, average_over=average) * -1\n",
        "    rewards[:,1] = rewards[:,1] * 25\n",
        "    plt.plot(rewards.sum(axis=1), label=filename)\n",
        "\n",
        "  plt.rcParams[\"figure.figsize\"] = [10,6]\n",
        "  plt.yscale('log')\n",
        "  plt.legend()\n",
        "  plt.ylabel('reward per episode')\n",
        "  plt.xlabel(f'{average} episode average')\n",
        "  plt.title(f'Rewards for {title},  beta scale 25')\n",
        "  plt.savefig(gen_path(title, 25) + '.png', bbox_inches='tight')\n",
        "  print(gen_path(filename, 25) + '/graph.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OZWNCuel-48"
      },
      "source": [
        "## Test Tabular"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv_4m3kAwmu2"
      },
      "source": [
        "def tabular_learn_with_backup(algo, filename, beta_scale=1, num_batches=100, batch_size=1000):\n",
        "  directory_path = gen_path(filename, beta_scale)\n",
        "  if not os.path.exists(directory_path):\n",
        "      os.mkdir(directory_path)\n",
        "  algo.env.beta = algo.env.beta * beta_scale\n",
        "  rewards = np.zeros((0,2))\n",
        "  for batch in range(num_batches):\n",
        "    print(\"\\nbatch \" + str(batch))\n",
        "    rewards = np.append(rewards, algo.learn(batch_size) * (1, 1/beta_scale), axis=0)\n",
        "    if hasattr(algo, 'q_table'):\n",
        "      model = algo.q_table\n",
        "    else:\n",
        "      model = algo.v_table\n",
        "    with open(directory_path + \"/model.txt\", 'w+') as f:\n",
        "      f.write(json.dumps(model))\n",
        "    np.savetxt(directory_path + \"/rewards.txt\", rewards)\n",
        "\n",
        "def tabular_load_model(q_learning, filename, beta_scale=1):\n",
        "  directory_path = gen_path(filename, beta_scale)\n",
        "  if not os.path.exists(directory_path):\n",
        "    print(\"no such file\")\n",
        "  with open(directory_path + \"/model.txt\") as f:\n",
        "    q_learning.v_table = json.loads(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RREJH1NXl5Mq"
      },
      "source": [
        "if RUN:\n",
        "  filename = \"N-step\"\n",
        "  beta = 25\n",
        "  tabular_learn_with_backup(algo=NstepVLearningParallel(),filename=filename,beta_scale=beta, num_batches=20,batch_size=100)\n",
        "  graph_results(filename=filename, beta=beta,x_min_comp=10, title=filename, average=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dVPVZzwxMjH"
      },
      "source": [
        "## Test Neural Nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYJk-_v_652I"
      },
      "source": [
        "def net_learn_with_backup(algo, filename, beta_scale=1, num_batches=100, batch_size=1000):\n",
        "  directory_path = gen_path(filename, beta_scale)\n",
        "  if not os.path.exists(directory_path):\n",
        "      os.mkdir(directory_path)\n",
        "  algo.env.beta = algo.env.beta * beta_scale\n",
        "  rewards = np.zeros((0,2))\n",
        "  print(algo.env.action_space)\n",
        "  for batch in range(num_batches):\n",
        "    print(\"\\nbatch \" + str(batch))\n",
        "    rewards = np.append(rewards, algo.learn(batch_size) * (1, 1/beta_scale), axis=0)\n",
        "    torch.save({'network': algo}, directory_path + \"/network.txt\")\n",
        "    np.savetxt(directory_path + \"/rewards.txt\", rewards)\n",
        "\n",
        "def net_load_model(filename, beta_scale=1):\n",
        "  directory_path = gen_path(filename, beta_scale)\n",
        "  if not os.path.exists(directory_path):\n",
        "    print(\"no such file\")\n",
        "  return torch.load(directory_path + \"/network.txt\")['network']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdT-3nRpXWZx"
      },
      "source": [
        "if RUN:\n",
        "  filename = \"A2C\"\n",
        "  beta = 25\n",
        "  # net_learn_with_backup(algo=A2C(),filename=filename,beta_scale=beta, num_batches=10,batch_size=20)\n",
        "  graph_results(filename=filename, beta=beta, x_min_comp=3, title=filename, average=5)\n",
        "  #graph_results(filename=filename, beta=beta, average=2) # for cartpole testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K-zTyPRHunz"
      },
      "source": [
        "## Neural network sanity test code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m3iXAFcVJcM"
      },
      "source": [
        "# import gym\n",
        "# import sys\n",
        "# import torch  \n",
        "# import gym\n",
        "# import numpy as np  \n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torch.nn.functional as F\n",
        "# from torch.autograd import Variable\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# class ActorCritic(nn.Module):\n",
        "#   def __init__(self, state_space, action_space_list):\n",
        "#     super(ActorCritic, self).__init__()\n",
        "#     self.shared_layers = nn.Sequential(nn.Linear(state_space, 256), nn.ReLU())\n",
        "#     self.critic_linear = nn.Linear(256, 1)\n",
        "#     self.actors_linear = nn.ModuleList([nn.Linear(256, num_actions) for num_actions in action_space_list])\n",
        "    \n",
        "#   def forward(self, state):\n",
        "#     state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
        "#     shared_nodes = self.shared_layers(state)\n",
        "#     critic = self.critic_linear(shared_nodes)\n",
        "#     policy_blah = []\n",
        "#     for actor_shizle in self.actors_linear:\n",
        "#       policy_blah.append(F.softmax(actor_shizle(shared_nodes), dim=1))\n",
        "#     return policy_blah, critic\n",
        "\n",
        "# class ReinforceNetwork(nn.Module):\n",
        "#   def __init__(self, state_space, action_space_list):\n",
        "#     super(ReinforceNetwork, self).__init__()\n",
        "#     self.shared_layers = nn.Sequential(nn.Linear(state_space, 256), nn.ReLU())\n",
        "#     self.actors_linear = nn.ModuleList([nn.Linear(256, num_actions) for num_actions in action_space_list])\n",
        "    \n",
        "#   def forward(self, state):\n",
        "#     state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
        "#     shared_nodes = self.shared_layers(state)\n",
        "#     policy_blah = []\n",
        "#     for actor_shizle in self.actors_linear:\n",
        "#       policy_blah.append(F.softmax(actor_shizle(shared_nodes), dim=1))\n",
        "#     return policy_blah\n",
        "\n",
        "# class MyCartPole2():\n",
        "#   def __init__(self):\n",
        "#     self.env = gym.make(\"CartPole-v0\")\n",
        "#     self.env2 = gym.make(\"CartPole-v0\")\n",
        "#     self.state_space = self.env.observation_space.shape[0] * 2\n",
        "#     self.action_space = [2, 2, 2]\n",
        "#     self.beta = 0\n",
        "\n",
        "#   def reset(self):\n",
        "#     return np.append(self.env.reset(), self.env2.reset())\n",
        "\n",
        "#   def step(self, action):\n",
        "#     a1, b1, c1, _ = self.env.step(action[1])\n",
        "#     a2, b2, c2, _ = self.env2.step(action[2])\n",
        "#     return np.append(a1, a2), np.array([b1, b2]), c1 or c2\n",
        "\n",
        "# class MyCartPole():\n",
        "#   def __init__(self):\n",
        "#     self.env = gym.make(\"CartPole-v0\")\n",
        "#     self.state_space = self.env.observation_space.shape[0]\n",
        "#     self.action_space = [2]\n",
        "#     self.beta = 0\n",
        "\n",
        "#   def reset(self):\n",
        "#     return self.env.reset()\n",
        "\n",
        "#   def step(self, action):\n",
        "#     a, b, c, _ = self.env.step(action[0])\n",
        "#     return a, np.array([b, 0]), c\n",
        "\n",
        "\n",
        "# def graph_results(filename, beta=1, average=5): \n",
        "#   rewards = load_rewards(filename, beta, average)\n",
        "#   plt.plot(rewards[:,0])\n",
        "#   plt.plot(rewards[:,1])\n",
        "#   plt.rcParams[\"figure.figsize\"] = [10,6]\n",
        "#   plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}